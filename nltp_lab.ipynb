{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df: pd.DataFrame = pd.read_csv('data/sentiment_analysis.csv')\n",
    "corpus = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NLP\n",
    "\n",
    "NLP (Natural Language Processing) is all about enabling computers to understand, interpret, and work with human language in a meaningful way. Think about things like sentiment analysis, machine translation, chatbots, or information retrieval—NLP powers all these applications and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is breaking text into smaller pieces, like sentences or words. It’s one of the first steps in processing language data.\n",
    "\n",
    "- **Sentence Tokenization**: Splits text into sentences.  \n",
    "- **Word Tokenization**: Splits sentences into words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             What a great day!!! Looks like dream.\n",
      "1    I feel sorry, I miss you here in the sea beach\n",
      "Name: text, dtype: object\n",
      "[['What', 'a', 'great', 'day', '!', '!', '!', 'Looks', 'like', 'dream', '.'], ['I', 'feel', 'sorry', ',', 'I', 'miss', 'you', 'here', 'in', 'the', 'sea', 'beach']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import NLTKWordTokenizer\n",
    "\n",
    "tokenizer = NLTKWordTokenizer()\n",
    "tokens = tokenizer.tokenize_sents(corpus[0:2])\n",
    "\n",
    "print(corpus[0:2])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "\n",
    "Text Vectorization The techniques which convert text into features are generally known as \"Text Vectorization techniques\", because they all aim to convert text into vectors (array) that can then be fed to a machine learning model.\n",
    "\n",
    "| Sentence                  | Tokens                               | One-Hot Encoding Vector |\n",
    "|---------------------------|--------------------------------------|--------------------------|\n",
    "| \"The cat sat in the hat\"  | [\"The\", \"cat\", \"sat\", \"in\", \"hat\"] | [1, 1, 1, 1, 1, 0]    |\n",
    "| \"The cat with the hat\"    | [\"The\", \"cat\", \"with\", \"the\", \"hat\"] | [1, 1, 0, 0, 1, 1]  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['!', ',', '.', 'I', 'Looks', 'What', 'a', 'beach', 'day', 'dream', 'feel', 'great', 'here', 'in', 'like', 'miss', 'sea', 'sorry', 'the', 'you'] \n",
      "--------------------\n",
      "Sentence 0: ['What', 'a', 'great', 'day', '!', '!', '!', 'Looks', 'like', 'dream', '.']\n",
      "Vector 0: [1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Sentence 1: ['I', 'feel', 'sorry', ',', 'I', 'miss', 'you', 'here', 'in', 'the', 'sea', 'beach']\n",
      "Vector 1: [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(word for sentence in tokens for word in sentence))\n",
    "print(f'Vocabulary: {vocabulary} \\n{\"-\"*20}')\n",
    "\n",
    "print('Sentence 0:',tokens[0])\n",
    "print('Vector 0:',[1 if word in tokens[0] else 0 for word in vocabulary])\n",
    "\n",
    "print('Sentence 1:',tokens[1])\n",
    "print('Vector 1:',[1 if word in tokens[1] else 0 for word in vocabulary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech (POS) Tagging\n",
    "\n",
    "Assigns parts of speech (e.g., noun, verb, adjective) to each word.\n",
    "\n",
    "![alt text](<images/Pasted image 20241109035814.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John's big idea isn't all that bad : [('John', 'NNP'), (\"'s\", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]\n",
      "John's big idea isn't all that bad : [('John', 'NOUN'), (\"'s\", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'), (\"n't\", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Riyadh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Riyadh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"John's big idea isn't all that bad :\",pos_tag(word_tokenize(\"John's big idea isn't all that bad.\")) )\n",
    "\n",
    "print(\"John's big idea isn't all that bad :\", pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"), tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "Text normalization is a key step in preparing raw text data for analysis. It involves transforming text into a consistent format, making it easier for models to interpret and analyze. This process reduces noise and variability, ensuring that different forms of the same word or expression are treated consistently in NLP tasks.\n",
    "\n",
    "![alt text](<images/Pasted image 20241021005733.png>)\n",
    "\n",
    "A **text preprocessing pipeline** cleans and standardizes text by **lowercasing**, **removing repetitions and punctuation**, and **normalizing words** with stemming or lemmatization. This improves consistency and prepares the text for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what a great day!!! looks like dream.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove repetitive sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'writing report cards  soooo tired but what an amazing day. check it out on fb soon!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'writing report cards  so tired but what an amazing day. check it out on fb soon!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.sub(r\"(.)\\1{2,}\", r\"\\1\", corpus[167])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Characters removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What a great day Looks like dream'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using regex to remove unwanted chars bt negating the selected chars\n",
    "re.sub(r'[^a-zA-Z ]', '', corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization & Stemming\n",
    "\n",
    "![alt text](<images/Pasted image 20241021005855.png>)\n",
    "\n",
    "These techniques simplify vocabulary and reduce feature size, but they impact the **accuracy of word meaning** and can affect the **contextual interpretation**. For example, stemming may group words too aggressively, leading to errors in sentiment analysis, while lemmatization maintains more semantic clarity.\n",
    "\n",
    "| Technique         | Pros                                                          | Cons                                                                 |\n",
    "| ----------------- | ------------------------------------------------------------- | -------------------------------------------------------------------- |\n",
    "| **Stemming**      | - Faster, computationally light                               | - Less accurate, may distort meaning (e.g., \"universal\" → \"univers\") |\n",
    "| **Lemmatization** | - More accurate, retains meaning (e.g., \"am, are, is\" → \"be\") | - Slower, requires more computation and linguistic resources         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cards'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[167].split(' ')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'card'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(corpus[167].split(' ')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'writing report cards  soooo tired but what an amazing day. check it out on fb soon!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amaz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(corpus[167].split(' ')[9]) # amazing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "**Stopwords** are common words in a language, such as \"the,\" \"is,\" \"in,\" and \"and,\" which typically carry minimal semantic value and do not contribute significantly to the meaning of the text. In natural language processing (NLP), removing stopwords is a common preprocessing step to reduce the noise in text data, allowing models to focus on more meaningful terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Why Remove Stopwords?\n",
    "- **Enhances Model Efficiency**: Removing stopwords reduces the vocabulary size, making the model more efficient and reducing computational costs.\n",
    "- **Improves Relevance**: Helps to focus on words with higher semantic importance, which can lead to better model accuracy in tasks like text classification, search relevance, and topic modeling.\n",
    "\n",
    "Stopwords removal is generally useful but should be carefully applied based on the context, as some stopwords may carry meaning in specific tasks, like sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Word embeddings are a way of representing words as vectors in a multi-dimensional space, where the distance and direction between vectors reflect the similarity and relationships among the corresponding words.\n",
    "\n",
    "### Two approaches to word embeddings\n",
    "\n",
    "#### Frequency-based embeddings\n",
    "\n",
    "Frequency-based embeddings refer to word representations that are derived from the frequency of words in a corpus. These embeddings are based on the idea that the importance or significance of a word can be inferred from how frequently it occurs in the text.\n",
    "\n",
    "##### Bag-of-words\n",
    "\n",
    "Bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. Machine learning algorithms cannot work with raw text directly; the text must be converted into well defined fixed-length(vector) numbers.\n",
    "\n",
    "Example:\n",
    "1. “The cat sat” \n",
    "2. “The cat sat in the hat” \n",
    "3. “The cat with the hat”\n",
    "\n",
    "![alt text](<images/Pasted image 20241021005545.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "TF-IDF is used to emphasize words that are common within a specific document but relatively rare across the entire corpus, helping to highlight key terms for each document.\n",
    "\n",
    "The TF-IDF score for a term is calculated as:\n",
    "\n",
    "$$\n",
    "\\textit{TF-IDF} = TF \\times IDF\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{Let}\\ t&=\\text{Term} \\\\\n",
    "    \\text{Let}\\ IDF&=\\text{Inverse Document Frequency} \\\\\n",
    "    \\text{Let}\\ TF&=\\text{Term Frequency} \\\\[2em]\n",
    "    TF \\:&=\\: \\frac{\\text{term frequency in document}}{\\text{total words in document}} \\\\[1em]\n",
    "    IDF(t) \\:&=\\: \\log_2\\left(\\frac{\\text{total documents in corpus}}{\\text{documents with term}}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Note**: To avoid divide-by-zero errors, add 1 to all counters if a term is absent in the corpus.\n",
    "\n",
    "\n",
    "###### Step 1: Calculate Term Frequencies (TF)\n",
    "\n",
    "To find Term Frequency (TF), divide the count of each term by the total number of terms in the document.\n",
    "\n",
    "Sample documents:\n",
    "\n",
    "1. **Document 1 (D1)**: \"The cat sat\" – Total terms: 3\n",
    "2. **Document 2 (D2)**: \"The cat sat in the hat\" – Total terms: 6\n",
    "3. **Document 3 (D3)**: \"The cat with the hat\" – Total terms: 5\n",
    "\n",
    "| Term |  TF in Document 1 (D1)   |     TF in Document 2 (D2)      |  TF in Document 3 (D3)  |\n",
    "| :--: | :----------------------: | :----------------------------: | :---------------------: |\n",
    "| the  | $$ \\frac{1}{3} = 0.33 $$ | $$ \\frac{2}{6} \\approx 0.33 $$ | $$ \\frac{1}{5} = 0.2 $$ |\n",
    "| cat  | $$ \\frac{1}{3} = 0.33 $$ | $$ \\frac{1}{6} \\approx 0.17 $$ | $$ \\frac{1}{5} = 0.2 $$ |\n",
    "| sat  | $$ \\frac{1}{3} = 0.33 $$ | $$ \\frac{1}{6} \\approx 0.17 $$ |            0            |\n",
    "|  in  |            0             | $$ \\frac{1}{6} \\approx 0.17 $$ |            0            |\n",
    "| hat  |            0             | $$ \\frac{1}{6} \\approx 0.17 $$ | $$ \\frac{1}{5} = 0.2 $$ |\n",
    "| with |            0             |               0                | $$ \\frac{1}{5} = 0.2 $$ |\n",
    "\n",
    "###### Step 2: Calculate Inverse Document Frequency (IDF)\n",
    "\n",
    "Calculate the IDF for each term:\n",
    "\n",
    "- **the**: $$\\log_2\\left(\\frac{3}{3}\\right) = 0$$\n",
    "- **cat**: $$\\log_2\\left(\\frac{3}{3}\\right) = 0$$\n",
    "- **sat**: $$\\log_2\\left(\\frac{3}{2}\\right) \\approx 0.585$$\n",
    "- **in**: $$\\log_2\\left(\\frac{3}{1}\\right) \\approx 1.585$$\n",
    "- **hat**: $$\\log_2\\left(\\frac{3}{2}\\right) \\approx 0.585$$\n",
    "- **with**: $$\\log_2\\left(\\frac{3}{1}\\right) \\approx 1.585$$\n",
    "\n",
    "###### Step 3: Calculate TF-IDF Scores\n",
    "\n",
    "Multiply the TF and IDF values for each term in each document.\n",
    "\n",
    "| Term | TF-IDF (D1)                | TF-IDF (D2)                | TF-IDF (D3)                |\n",
    "| ---- | ---------------------------| ---------------------------| ---------------------------|\n",
    "| the  | $$0 \\times 0.33 = 0$$      | $$0 \\times 0.33 = 0$$      | $$0 \\times 0.2 = 0$$       |\n",
    "| cat  | $$0 \\times 0.33 = 0$$      | $$0 \\times 0.17 = 0$$      | $$0 \\times 0.2 = 0$$       |\n",
    "| sat  | $$0.585 \\times 0.33 \\approx 0.193$$ | $$0.585 \\times 0.17 \\approx 0.1$$ | 0           |\n",
    "| in   | $$1.585 \\times 0 = 0$$     | $$1.585 \\times 0.17 \\approx 0.27$$ | 0           |\n",
    "| hat  | $$0.585 \\times 0 = 0$$     | $$0.585 \\times 0.17 \\approx 0.1$$ | $$0.585 \\times 0.2 \\approx 0.117$$ |\n",
    "| with | $$1.585 \\times 0 = 0$$     | 0                            | $$1.585 \\times 0.2 \\approx 0.317$$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### N-grams\n",
    "\n",
    "An N-gram represents a sequence of N words (word level) or N characters (character level) in a text. By capturing these sequences, N-grams help preserve word connections and contextual relationships, allowing for a more generalized understanding of text.\n",
    "\n",
    "\n",
    "![alt text](<images/Pasted image 20241021005447.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is')\n",
      "('is', 'a')\n",
      "('a', 'foo')\n",
      "('foo', 'bar')\n",
      "('bar', 'sentences')\n",
      "('sentences', 'and')\n",
      "('and', 'I')\n",
      "('I', 'want')\n",
      "('want', 'to')\n",
      "('to', 'ngramize')\n",
      "('ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'this is a foo bar sentences and I want to ngramize it'\n",
    "\n",
    "n = 2\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for grams in sixgrams:\n",
    "  print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Prediction-based embeddings\n",
    "\n",
    "Prediction-based embeddings are word representations derived from models that are trained to predict certain aspects of a word's context or neighboring words. Unlike frequency-based embeddings that focus on word occurrence statistics, prediction-based embeddings capture semantic relationships and contextual information, providing richer representations of word meanings.\n",
    "\n",
    "##### Word2Vec\n",
    "\n",
    "Word2Vec embeddings place similar words near each other in vector space, capturing relationships between them. For example, the model can understand that 'man' is to 'woman' as 'king' is to 'queen,' showing how words relate through meaning. This ability to recognize patterns and analogies is a big advantage of Word2Vec.\n",
    "\n",
    "![alt text](<images/Pasted image 20241109040300.png>)\n",
    "\n",
    "Word2Vec consists of two main models for generating vector representations: Continuous Bag of Words (CBOW) and Continuous Skip-gram. \n",
    "\n",
    "![alt text](<images/Pasted image 20241021001505.png>)\n",
    "\n",
    "In the context of Word2Vec, the **Continuous Bag of Words (CBOW) model** aims to predict a target word based on its surrounding context words within a given window. It uses the context words to predict the target word, and the learned embeddings capture semantic relationships between words.\n",
    "\n",
    "The **Continuous Skip-gram model**, on the other hand, takes a target word as input and aims to predict the surrounding context words.\n",
    "\n",
    "###### Limitations of Word2Vec\n",
    "\n",
    "Do you know the Ozone Layer?\n",
    "\n",
    "Using **Word2Vec**, let's examine the associations for \"Ozone\":\n",
    "\n",
    "| Word  | Human | Food | Liquid | Chemical |\n",
    "| ----- | ----- | ---- | ------ | -------- |\n",
    "| Cake  | 0.0   | 0.9  | 0.1    | 0.0      |\n",
    "| Juice | 0.0   | 0.5  | 0.9    | 0.0      |\n",
    "| Acid  | 0.0   | 0.0  | 0.1    | 0.9      |\n",
    "| Ozone | 0.0   | 0.7  | 0.0    | 0.1      |\n",
    "\n",
    "![alt text](<images/Pasted image 20241009234647.png>)\n",
    "\n",
    "\"Ozone\" is classified as Food\n",
    "\n",
    "This example shows that **Word2Vec can make associations** based on statistical relationships in text, but it lacks real-world understanding of concepts. Here, \"Ozone\" is incorrectly associated with \"Food,\" despite being a gas.\n",
    "\n",
    "\n",
    "\n",
    "Another limitation of Word2Vec is its inability to distinguish specific subtypes within a category. For instance, **different types of coffee** like espresso and cappuccino may be interpreted as nearly identical due to the small cosine distance between them, even though they are distinct.\n",
    "\n",
    "![alt text](<images/Pasted image 20241005065755.png>)\n",
    "\n",
    "This highlights that while Word2Vec captures statistical patterns, it doesn’t fully understand nuanced distinctions between items within a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we train a word embedding using the Brown Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Riyadh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "train_set = brown.sents()[:10000]\n",
    "model = gensim.models.Word2Vec(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might take some time to train the model. So, after it is trained, it can be saved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('brown.embedding')\n",
    "new_model = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be the list of words with their embedding. We can easily get the vector representation of a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_model.wv['university'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some supporting functions already implemented in Gensim to manipulate with word embeddings. For example, to compute the cosine similarity between 2 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.wv.similarity('university','school') > 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the pre-trained model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK includes a pre-trained model which is part of a model that is trained on 100 billion words from the Google News Dataset. The full model is from https://code.google.com/p/word2vec/ (about 3 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     C:\\Users\\Riyadh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping models\\word2vec_sample.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('word2vec_sample')\n",
    "\n",
    "from nltk.data import find\n",
    "\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pruned the model to only include the most common words (~44k words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43981"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each word is represented in the space of 300 dimensions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model['university'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finding the top n words that are similar to a target word is simple. The result is the list of n words with the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003918290138245),\n",
       " ('faculty', 0.6780906319618225),\n",
       " ('undergraduate', 0.6587095856666565)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['university'], topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a word that is not in a list is also supported, although, implementing this by yourself is simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"King - Man + Woman\" is close to \"Queen\" and \"Germany - Berlin + Paris\" is close to \"France\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 0.7884091138839722)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman','king'], negative=['man'], topn = 1)\n",
    "model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GloVe\n",
    "\n",
    "Unlike the Word2Vec models (CBOW and Skip-gram), which focus on predicting context words given a target word or vice versa, GloVe uses a different approach that involves optimizing word vectors based on their co-occurrence probabilities. The training process is designed to learn embeddings that effectively capture the semantic relationships between words.\n",
    "\n",
    "[Continue reading](https://github.com/stanfordnlp/glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BERT\n",
    "\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)** is a model that creates context-aware embeddings by reading text in both directions. Unlike simpler embeddings, BERT can understand words based on the surrounding words, handling polysemy (multiple meanings) and capturing deep semantic relationships.\n",
    "\n",
    "- **Bidirectional**: Analyzes each word with both left and right context, providing a fuller understanding of meaning.\n",
    "- **Handles Polysemy and Synonymy**: Differentiates meanings based on context, useful for tasks like question answering and sentiment analysis.\n",
    "- **Pretrained**: Trained on massive text data, making it adaptable to various language tasks.\n",
    "\n",
    "BERT's embeddings are widely used in NLP tasks where deep contextual understanding is required.\n",
    "\n",
    "[Continue reading](https://huggingface.co/docs/transformers/en/model_doc/bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Comparison of Word Embedding Techniques\n",
    "\n",
    "| Feature                       | Count Vectorization | TF-IDF Vectorization | Word2Vec (CBOW, Skip-gram) | GloVe |\n",
    "| ----------------------------- | ------------------- | -------------------- | -------------------------- | ----- |\n",
    "| **Interpretable**             | ✅                   | ✅                    | ❌                          | ❌     |\n",
    "| **Captures Semantic Meaning** | ❌                   | ❌                    | ✅                          | ✅     |\n",
    "| **Sparse Representation**     | ✅                   | ✅                    | ❌                          | ❌     |\n",
    "| **Handles Large Vocabulary**  | ❌                   | ❌                    | ✅                          | ✅     |\n",
    "| **Context-Aware**             | ❌                   | ❌                    | ✅                          | ✅     |\n",
    "| **Easy to Compute**           | ✅                   | ✅                    | ❌                          | ❌     |\n",
    "| **Suitable for Short Text**   | ✅                   | ✅                    | ✅                          | ✅     |\n",
    "| **Requires Large Dataset**    | ❌                   | ❌                    | ✅                          | ✅     |\n",
    "| **Handles Synonymy**          | ❌                   | ❌                    | ✅                          | ✅     |\n",
    "| **Handles Polysemy**          | ❌                   | ❌                    | ~                          | ~     |\n",
    "\n",
    "In this table:\n",
    "- **Polysemy Handling** (`~`): Word2Vec and GloVe can partially handle polysemy by capturing context to an extent, but they don’t fully differentiate meanings like contextual embeddings (e.g., BERT).\n",
    "\n",
    "---\n",
    "# Conclusion and Next Steps\n",
    "\n",
    "In this guide, we've covered the essential components of Natural Language Processing, from text preprocessing to advanced word embeddings. Understanding these foundational steps is crucial for building powerful NLP models and tackling real-world language tasks.\n",
    "\n",
    "This repository will include Jupyter notebooks to provide hands-on practice with these concepts. These notebooks will guide you through practical projects, such as building a sentiment classifier or performing named entity recognition. You'll also find tools and techniques for preprocessing text, implementing embeddings, and evaluating NLP models.\n",
    "\n",
    "### What’s Next?\n",
    "1. **Deepen Your Knowledge**: Continue exploring advanced topics in NLP, such as dependency parsing, topic modeling, and more sophisticated embeddings like contextualized embeddings (e.g., BERT).\n",
    "2. **Practice with Real Data**: Applying these techniques on real datasets is the best way to solidify your understanding.\n",
    "3. **Explore Other NLP Libraries**: After mastering NLTK, consider learning SpaCy or the Hugging Face Transformers library for modern NLP workflows.\n",
    "\n",
    "With these resources and hands-on practice, you’ll be well-prepared to tackle more complex NLP projects and keep advancing your skills.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
